{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "sys.path.append(\"../../marineHeatWaves/\")\n",
    "sys.path.append(\"../analysis/physiology/\")\n",
    "\n",
    "import sys\n",
    "import time\n",
    "oldstderr = sys.stderr\n",
    "sys.stderr = open('log.txt', 'w')\n",
    "\n",
    "\n",
    "import ipywidgets as ipw\n",
    "\n",
    "import sentinelsat\n",
    "import satpy\n",
    "from pyresample import create_area_def, AreaDefinition\n",
    "from satpy.writers import compute_writer_results\n",
    "import requests\n",
    "from requests import auth, get\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import rasterio as rio\n",
    "\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from cartopy import crs as ccrs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tqdm\n",
    "\n",
    "# coda_auth = auth.HTTPBasicAuth('tonycan', os.environ['ss_pass'])\n",
    "\n",
    "\n",
    "from shapely import geometry, wkt\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask import delayed\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from functools import partial\n",
    "from itertools import zip_longest, cycle, chain\n",
    "\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ss_pass'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pool = Pool(cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole Record, One Isolate Sentinel 3 Validation\n",
    "\n",
    "I think I'm going to take the entire Sentinel 3 Record over a single spot and look at the relationship between chlorophyll + performance throughout that period, perhaps punctuated by s3?\n",
    "\n",
    "The general question here is whether chlorophyll-a patterns can be explained by performance, where MHWs are a particular subset of \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plankton = pd.read_csv(\"../data/Phytoplankton_temperature_growth_rate_dataset_2016_01_29/traits_derived_2016_01_29.csv\", engine='python')\n",
    "\n",
    "plankton = plankton[\n",
    "    (plankton.habitat == 'marine') & \n",
    "    (plankton.curvequal == 'good')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll only use the NE-Pacific isolate (#240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_isolate = plankton[plankton['isolate.code'] == 240]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Entire Sentinel-3 Record for this Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_api_historical = sentinelsat.SentinelAPI(\n",
    "    'tonycan', \n",
    "    os.environ['ss_pass'], \n",
    "    api_url = 'https://codarep.eumetsat.int/'\n",
    ")\n",
    "s3_api_recent = sentinelsat.SentinelAPI(\n",
    "    'tonycan', \n",
    "    os.environ['ss_pass'], \n",
    "    api_url = 'https://coda.eumetsat.int/'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isolate_mhw_wkt = wkt.dumps(\n",
    "    geometry.Polygon.from_bounds(\n",
    "        *geometry.Point([\n",
    "            chosen_isolate['isolation.longitude'],\n",
    "            chosen_isolate['isolation.latitude']\n",
    "        ]).buffer(2).bounds\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryParams = dict(\n",
    "    area = isolate_mhw_wkt, \n",
    "    producttype='OL_2_WRR___',\n",
    "    date = ('20160101', '20191231')\n",
    ")\n",
    "\n",
    "qr_historical = s3_api_historical.query(**queryParams)\n",
    "qr_recent = s3_api_recent.query(**queryParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_result_combined = pd.concat([\n",
    "    s3_api_historical.to_geodataframe(qr_historical),\n",
    "    s3_api_recent.to_geodataframe(qr_recent)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "[ax.axvline(x.beginposition) for _, x in s3_result_combined.iterrows()];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there's a data gap in 2018, I can't really explain this. Let's just do the historical data from `codarep`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_result_historical = s3_api_historical.to_geodataframe(qr_historical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(image, path, auth):\n",
    "    _, image = image\n",
    "    r = get(image.link, auth=auth)\n",
    "    exitpath = os.path.join(path, f'{image.title}.zip')\n",
    "    with open(exitpath, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return(exitpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coda_auth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7d4dde015a02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdownload_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/tmp/nepacs3/historical/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdownload_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoda_auth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'coda_auth' is not defined"
     ]
    }
   ],
   "source": [
    "download_location = '/tmp/nepacs3/historical/'\n",
    "os.makedirs(download_location, exist_ok=True)\n",
    "download_func = partial(download_image, path=download_location, auth=coda_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pool.map(download_func, s3_result_historical.iterrows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unzip all of the data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$download_location\"\n",
    "ls ${1}*.zip | parallel -t unzip  > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process `chl_oc4me` and `chl_nn` data\n",
    "\n",
    "**Note**: I initially tried to run the following code in parallel in this notebook, but ran into issues with dask/satpy locality and pickling rasterio objects. Spent the better part of a day working that out, and couldn't figure it out. The final solution was to break the `reproject_and_save` function out into its own module (`reproject_s3.py`) and ran that using `gnu parallel` as follows: \n",
    "\n",
    "    find . -name \"*.SEN3\" -type d | parallel --no-notice -k  \"python /home/ec2-user/mhw_stressviz/validation/reproject_s3.py {}\"\n",
    "    \n",
    "I will keep the old stuff here for posterity + reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reproject_and_save(img_directory, projection=ccrs.Mercator()):\n",
    "#     chl_data = ['chl_oc4me', 'chl_nn']\n",
    "\n",
    "#     _sc = satpy.Scene(glob(img_directory+\"/*\") , reader='olci_l2', )\n",
    "\n",
    "#     _sc.load(chl_data)\n",
    "\n",
    "#     outpath = os.path.join(img_directory, f\"{_sc.attrs['start_time']:%Y%m%d}.cf\")\n",
    "\n",
    "#     _sc_resampled = _sc.resample(\n",
    "#             _sc[chl_data[0]].attrs['area'].compute_optimal_bb_area(projection.proj4_params)\n",
    "#     )\n",
    "    \n",
    "# return(_sc_resampled.save_datasets(writer='cf', filename=outpath, compute=False))\n",
    "\n",
    "    \n",
    "# #     delayed = []\n",
    "# #     for d in chl_data:\n",
    "# #         _sc_resampled = _sc.resample(\n",
    "# #             _sc[d].attrs['area'].compute_optimal_bb_area(projection.proj4_params)\n",
    "# #         )\n",
    "# #         outpath = os.path.join(img_directory, f\"{d}-{_sc.attrs['start_time']:%Y%m%d}.cf\")\n",
    "        \n",
    "# #         delayed.append(_sc_resampled.save_dataset(d, writer='cf', filename=outpath, compute=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproject_save_v2(path):\n",
    "    dss_dask = []\n",
    "    dss = ['chl_oc4me', 'chl_nn']\n",
    "    try:\n",
    "        _sc = satpy.Scene(glob(path+\"/*\") , reader='olci_l2')\n",
    "        _sc.load(dss)\n",
    "    except Exception as e:\n",
    "        return\n",
    "        \n",
    "    rs_sc = _sc.resample(_sc['chl_nn'].area.compute_optimal_bb_area(ccrs.Mercator().proj4_params))\n",
    "    for i in dss:\n",
    "        outpath = os.path.join(path, f\"{i}-{_sc.attrs['start_time']:%Y%m%d}.tif\")\n",
    "        dss_dask.append(rs_sc.save_dataset(i, outpath, writer='geotiff', compute=False))\n",
    "    return(dss_dask)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproject and Clip all outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = glob(os.path.join(download_location, \"*/*.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = list(map(get_date_from_file, processed_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(dates).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for file in processed_files:\n",
    "    try:\n",
    "        datasets.append(rio.open(file))\n",
    "    except Exception as e:\n",
    "        datasets.append(None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
